services:
  llm:
    image: ollama/ollama:0.11.11
    ports:
      - "11434:11434"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - llm_data:/root/.ollama

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - 3000:8080
    environment:
      - OLLAMA_BASE_URL=http://llm:11434
    depends_on:
      - llm

#   test:
#     image: nvidia/cuda:12.9.0-base-ubuntu22.04
#     command: nvidia-smi
#     deploy:
#       resources:
#         reservations:
#           devices:
#             - driver: nvidia
#               count: all
#               capabilities: [gpu]

volumes:
  llm_data: {}
  